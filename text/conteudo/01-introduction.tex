%!TeX root=../tese.tex
%("dica" para o editor de texto: este arquivo é parte de um documento maior)
% para saber mais: https://tex.stackexchange.com/q/78101/183146

%% ------------------------------------------------------------------------- %%
\chapter{Introduction}
\label{cap:introduction}

% Colocar uma prévia do trabalho em si, não só do contexto

Social networks have all but taken over contemporary daily life. From the
eponymous socializing, to reading news, to expressing ourselves, social media
has creeped into every corner of society. Most of its side-effects, it could be
argued, are positive (shortening distances, political accountability, social
organizing), but they are not perfect institutions.

Social media companies already face significant backlash for their questionable
business model and ethics. Cambridge Analytica's election meddling
\citep{cadwalladr_revealed_2018}, Facebook's subliminal experiments
\citep{kramer_experimental_2014}, YouTube's problem with disturbing content
marketed at kids \citep{dredge_youtubes_2016}, and Twitter's bot infestation
\citep{varol_online_2017} are just a few recent scandals that have put the
societal role of social media into question.

One particular controversy that has taken over public discourse around social
networks is the role that their algorithms might have in radicalizing users,
specially younger ones. The aforementioned experiments conducted by Facebook to
influence people's emotions and the proliferation of more than questionable
videos aimed at children on YouTube are instances that seem to corroborate the
notion that there is something fundamentally wrong with these companies'
algorithms.

News organizations, in general, have been skeptical of social networks.
Journalists and specialists alike argue that social media's algorithms
(specially recommender algorithms) are tuned to peddle conspiracy theories,
extremist views, and false information \citep{noauthor_mozilla_2021-1}. This
would be the source cause for a plethora of what they consider contemporary
evils: religious extremism, anti-democratic leaders, widespread depression among
teenagers, anti-science movements, etc.

This narrative, of course, has been questioned for a variety of reasons. Some
say that it is self serving: traditional news organizations are being displaced
by social media and it would be convenient for them to mine the public's trust
in them \citep{munger_right-wing_2020}. Others claim that these recommender
algorithms are not to blame for political polarization and that social networks
even have a tendency to favor more left-wing viewpoints
\citep{ledwich_algorithmic_2019}.

The debate around the role of recommender systems in social media radicalization
is still, unfortunately, too recent and based in anecdotes. Since its impacts
are all but universal, more quality research is vital to inform both the public
and opinion makers about if and how much recommendation algorithms influence
social media users.

This dissertation aims to further such research.

\section{Social Networks}
\label{sec:social_networks}

Social networking services, also referred to as social networks and social
media, are notoriously difficult to define. Some definitions might be too narrow
(excluding instant messaging services), while some might be too broad (including
technologies such as telephone networks). Most definitions
\citep{boyd_social_2007} include some common features:

\begin{itemize}
  \item Internet-based
  \item Focus on user-generated content
  \item Users have profiles
  \item Users can connect
\end{itemize}

While social-networking-like applications already existed in Usenet, Geocities,
launched in 1994, is usually regarded as the first major social network.
Friendster and Myspace followed in 2003, with Orkut and Facebook slightly
lagging behind in 2004. Each hit their peak at different moments and different
countries, but Facebook overtook all of them in 2009 when it became the most
popular social networking service in the world, still maintaining the title over
13 years latter at the moment of writing \citep{noauthor_biggest_nodate}.

Even though all aforementioned social networks are multimedia, that is, users
can post text, photos and videos, some of the most popular services focus on a
specific type of media. For instance, YouTube (2009) centers around videos,
WhatsApp (2009) and WeChat (2011) were originally designed for text-based
communication, and Instagram's (2010) main focus is photos.

Parallel to all other features and idiosyncrasies, there lay the recommendation
algorithms. While a few social networking services (e.g. WhatsApp) do not
recommend any content or profiles to the user, most do and, according to recent
studies, these recommendations have become the main drivers of interactions
\citep{stoica_algorithmic_2018}.

\section{Recommender Systems}
\label{sec:recommender_systems}

Recommender systems (sometimes called recommendation systems or recommender
algorithms) first appeared in 1992 under the name ``collaborative filtering'',
even though that term nowadays refers to a subclass of recommender systems
\citep{goldberg_using_1992}. The aim of such an algorithm is providing users
with personalized product or service recommendations, an essential task when
considering the ever increasing number of possible videos to watch, music to
listen, products to buy.

The input of a recommender system is usually information about the preferences
(ratings, likes/dislikes, watch time, etc.) of consumers for a set of items.
Preference information can be gathered from explicit behaviors (e.g. rating a
product in a scale ranging from 0 to 5 stars) or from implicit behaviors (e.g.
how much time the user lingers on a product's page). These data can be combined
with information about the user (age, political leaning, etc.) in order to
create the best possible representation of the user's preferences.

The output of these systems can come in the form of a prediction or a list of
recommended items. In the first case, the goal of the algorithm is approximating
the rating a user would attribute to a yet unrated item, while the second type
of output involves gathering the items that most likely would interest the user.
Simple recommender systems that suggest items similar to the one being queried
do not necessarily involve rating predictions, but it is common to have the list
of recommended items based on the ratings the algorithms estimated the user
would give to those items.

Most recommender systems fall into one of four categories according to the
filtering algorithm they use, that it, the strategy for generating predictions
or selecting the top-N items: content-based filtering, demographic filtering,
collaborative filtering, and hybrid filtering
\citep{bobadilla_recommender_2013}.

Content-based filtering leverages characteristics of the content in order to
generate the recommendations \citep{ricci_introduction_2011}. One such algorithm
might use the genres of watched movies in order to recommend new ones, while
another might analyze the sound signature of a song to recommend similar ones,
but, either way, all content-based systems establish a similarity between items
as a basis for recommendations. Analogously, demographic filtering uses
demographic data to establish a similarity between users and recommend items
positively rated by similar people.

Collaborative filtering algorithms also recommend items that similar users
liked, but, in this case, the similarity between users is based on past ratings
and not demographic information \citep{ricci_introduction_2011}. Hybrid
filtering usually mix collaborative methods with either content-based or
demographic filtering \citep{ricci_introduction_2011}.

As with other knowledge-based systems, recommendation algorithms have quickly
incorporated neural networks and other machine learning techniques over the past
few years. Even though the implementation of YouTube's recommendation algorithm
is a trade secret, it is known to gather enormous amounts of data about the
user's interaction with the website and to require Google's own TPUs in order to
be trained. It also involves two distinct steps: candidate generation (when the
billions of videos available on the platform are quickly narrowed down to a few
hundreds that might be relevant) and ranking (when the algorithm actually
attempts to predict the score a user would implicitly give to the candidate
videos) \citep{anonymous_improving_2022}.

Another relevant aspect of recommender systems that is well-exemplified by
YouTube is the use of balancing factors such as novelty, dispersity, and
stability \citep{zhao_recommending_2019}. In the case of Google's video giant,
there is a baked-in bias for recency, strongly favoring newer videos in
detriment of older content \citep{zhao_recommending_2019}.

From this kind of bias stems much debate: as recommender systems explode in
popularity, so does research regarding its shortcommings. User radicalization
and algorithmic bias (explicitly programmed or not) are hotly debated subjects
in the literature.

\section{Radicalization and Bias}
\label{sec:radicalization_bias}

Opinion polarization is far from a recent phenomenon, and social media is only
the most recent communication medium where it can be detected and studied. An
important question is whether it facilitates or attenuates polarization:
anecdotal evidence might suggest that social network structures incentivize
users to gather into antagonistic communities, but this could be a result of
people simply being more likely to express their preferences online, not of some
intrinsic property of social media.

One possible byproduct of polarization is radicalization. Despite not being
entirely different phenomena, these concepts deserve distinct levels of
attention. While polarization can be considered a natural part of democratic
discourse, radicalization only happens when certain conditions are met. UNESCO
defines radicalization as \citep{seraphin_youth_2017}:

\begin{itemize}
  \item The individual person's search for fundamental meaning, origin and
        return to a root ideology;
  \item The individual as part of a group's adoption of a violent form of
        expansion of root ideologies and related oppositionist objectives;
  \item The polarization of the social space and the collective construction of
        a threatened ideal 'us' against 'them,' where the others are dehumanized
        by a process of scapegoating.
\end{itemize}

The third point is of special importance to the distinction between polarization
and radicalization. The first might be a simple consequence of democratic
disagreements between opposing parties, but the latter involves a dehumanization
of the opposition, which can lead to extremism: radicalism so intense that the
only effective strategy is physically exterminating the opposition.

Understanding how polarization might lead to radicalization (and, ultimately, to
extremism) is, therefore, of paramount significance to cultivate healthy
democracies, specially in the digital age. Since most social networks, as of
this writing, are still poorly moderated, they allow users to be exposed to a
plethora of viewpoints, from benign to insidious, possibly configuring a
``pipeline of radicalization'' through which regular users end up radicalized by
coming into contact with extreme content \citep{ribeiro_auditing_2020}.

Of course this argument is still very much open for debate. As will be shown in
Chapter~\ref{cap:review}, researchers have found evidences both for and against
the pipeline hypothesis and even proposed other means though which social media
might help radicalize users (e.g. the supply and demand hypothesis
\citep{munger_right-wing_2020}). Despite all disagreements, one common point
addressed by most research is the role of recommendation algorithms in serving
users with radicalizing content.

Proponents of the pipeline hypothesis, for instance, argue that recommendation
systems, aiming to maximize content consumption, suggest items that reinforce
preconceived notions of the user and that play on fear and paranoia
\citep{ribeiro_auditing_2020}. Content that appears urgent and leaves the user
fearful (for their live, their community, or their identity) could be more
engaging and, therefore, might be more susceptible to being considered as
relevant by the algorithm.

Even if the pipeline hypothesis is correct, specifics of how much algorithms are
to blame for radicalization are still unknown and hard to pin down. Most
research about the subject focuses on specific platforms (like Twitter and
YouTube) and have severe limitations with regards to how much data those
companies make available, not to mention the constant changes made to the
algorithms over the years that might alter their radicalization properties.
Definitive evidence for one theory or another must, therefore, apply to
recommender systems in general and be predictive of how they work both in
controlled and real life scenarios.

% mesclar parágrafo abaixo

YouTube, for example, currently has over 2 billion monthly logged-in users
\citep{noauthor_youtube_nodate}, but it makes no significant effort to clarify
changes made to the algorithm or even whether they fulfill their promises of
reducing user exposure to radicalizing content. With more than 500 hours of
content being uploaded every minute \citep{noauthor_youtube_nodate}, if 1\% of
all videos can be considered radicalizing and the algorithm can detect 99\% of
them, that still leaves over 25.000 hours of brand new extremist content free to
spread on the platform every year. This goes to show that, in the scale that
these companies operate, even a small fraction of content might still be enough
to influence the overall recommendations made by the algorithm. It is also worth
noting that most of these platforms' efforts are concentrated in their parent
countries (usually the United States), so, even if they actually try and remove
the offending content, most of the non-English-speaking world would still not be
impacted by their policy changes.

Closely related to user radicalization is the subject of algorithmic bias.
YouTube, for example, has an explicit bias towards recency
\citep{covington_deep_2016}, meaning that more recent videos get "boosted" by
their recommendation algorithm. This is explicitly coded into the system, but
there are also implicit biases, learned by watching user behavior.

Stoica \citep{stoica_algorithmic_2018} studied Instagram profiles before and
after the implementation of their recommendation engine. They discovered that
male users had a slight predilection for following other men, while women
displayed no such preference and, as soon as the recommender system was
deployed, engagement with profiles of male users skyrocketed even though they
were the minority on the platform. The algorithm recognized and leveraged this
so called differentiated homophily effectively, but we might question whether or
not this should be the desired outcome of a good recommender system.

In social networks where recommendations are front and center, such as YouTube,
the algorithm could go from being a mere reflection of user preferences to
actively shaping user behavior. Hypothetically, a minoritary group of highly
engaged users with strong self-reinforcing consumption habits could tip the
scales of the algorithm and cause fringe content to be amplified; this is only
one way through which a radicalization pipeline could spontaneously form in a
social network.

\section{Research Goals}
\label{sec:research_goals}

As explained in the previous sections, social networks' recommendation
algorithms might play a significant role in radicalizing users. This could be,
at least in part, be a result of implicit and explicit biases in recommender
systems.

This dissertation aims to explore the radicalization pipeline hypothesis and,
more specifically, understand the mechanisms through which recommender systems
can end up learning or developing biases. The research developed here revolves
around the dynamical properties of recommender systems (i.e., the sequence of
items suggested to an arbitrary user over time) and how feedback loops can
create ``amplification pipelines'' inside these engines.

In short, the main motivator of this research is to test the pipeline hypothesis
in a setting where recommendation algorithms learn dynamically.

\section{Outline}
\label{sec:outline}

...
