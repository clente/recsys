%!TeX root=../tese.tex
%("dica" para o editor de texto: este arquivo é parte de um documento maior)
% para saber mais: https://tex.stackexchange.com/q/78101/183146

% Vamos definir alguns comandos auxiliares para facilitar.

% "textbackslash" é muito comprido.
% \newcommand{\sla}{\textbackslash}

% Vamos escrever comandos (como "make" ou "itemize") com formatação especial.
% \newcommand{\cmd}[1]{\textsf{#1}}

% Idem para packages; aqui estamos usando a mesma formatação de \cmd,
% mas poderíamos escolher outra.
% \newcommand{\pkg}[1]{\textsf{#1}}

% A maioria dos comandos LaTeX começa com "\"; vamos criar um
% comando que já coloca essa barra e formata com "\cmd".
% \newcommand{\ltxcmd}[1]{\cmd{\sla{}#1}}

\chapter{Static Analysis}
\label{cap:static}

As discussed in the previous chapters, understanding how social networks
recommend content to users is central to the debate around political
polarization and radicalization. There are many ways of exploring recommender
systems without examining their code \citep{}, from simulating their behavior
after careful observation to directly collecting recommendation data, but most
of them allow us to examine only one perspective of the algorithm at a time.
This means that studying a social network's recommendation technique has
inherent limitations.

Most of the algorithms currently employed by social media companies are trade
secrets. They are also subject to constant experimentation and tuning \citep{},
which might render worthless any research performed before an update to the
algorithm, no matter how carefull the design of the study was. YouTube, for
example, currently has over 2 billion monthly logged-in users \citep{}, but it
makes no significant effort to clarify changes made to the algorithm or even
whether they fulfill their promisses of reducing user exposure to radicalizing
content. With more than 500 hours of content being uploaded every minute
\citep{}, if 1\% of all videos can be considered radicalizing and the algorithm
can detect 99\% of them, that still leaves over 25.000 hours of brand new
extremist content free to spread on the platform every year. This goes to show
that, in the scale that these companies operate, even a small fraction of
content might still be enough to influence the overall recommendations made by
the algorithm. It is also worth noting that most of these platforms' efforts are
concentrated in their parent countries (usually the United States), so, even if
they actually try and remove the offending content, most of the
non-English-speaking world would still not be impacted by their policy changes.

This leads right into the goal of the present dissertation. With our experiments
we aim to make a tangible contribution to the field of recommender systems,
specifically how their design might (or might not) foster confinement dynamics
in the ``phase space'' of recommendations. If the main hypothesis is confirmed,
this could mean that even a relatively small fraction of the content can tip the
algorithm in its favor, amplifying their message, creating ``filter bubbles'',
and possibly sending users on a radicalization spiral if that topic is related
to politics or other contentious subjects.

\section{Datasets}
\label{sec:datasets03}

Before discussing any experiment, it is necessary to introduce the datasets used
to train the models. The main dataset explored in this dissertation is MovieLens
\citep{harper_movielens_2015}, a well-known set of movie reviews that has been
featured in many recommender system tutorials and papers for the past few years.
The full dataset, with 27,000,000 ratings applied to 58,000 movies, was enriched
by \citet{banik_movies_2017} with information about the movies' credits,
metadata, keywords, and links. In the end, because of technical limitations, the
dataset used in this chapter was sampled until 30,689 movies were left; this
allowed for faster experimentation and simpler plots.

The second dataset, used for validation purposes only, was the Book-Crossing
Dataset \citep{ziegler_book-crossing_2004}. Just like the enriched MovieLens,
this dataset contained entries for ratings (1,149,780) applied by users
(278,858) to items (271,379 books), and information about these items like
title, author, publisher, etc.

\section{Experiments}
\label{sec:experiments}

Some preliminary were conducted in order to gather some evidence in favor or
against the main hypothesis being tested. In total, fifteen different
recommendation models were trained and analyzed, with each plot below
representing one of these models. For simplicity's sake, even though all models
represented here are non-parametric, we still say they were ``trained'' because
the datasets used to generate recommendations are different.

The goal of these experiments was trying to identify if even a simple
recommendation algorithm could demonstrate some sort of bias towards a subset of
the items being recommended. More specifically, given an algorithm that cannot
be influenced by users' personal preferences, would the resulting recommender
system favor some kind of item? Excluding user information is important because,
as demonstrated by \citet{stoica_algorithmic_2018}, users might have their own
biases and these could get transferred on to the model; the objective here is
understanding the algorithm by itself without external influences.

To evaluate the recommendation models, at least qualitatively, the authors
plotted their ``recommendation profiles'': a summary of how many times an
arbitrary item is recommended overall. To create this profile, the algorithm is
asked to return the top-$n$ most similar items to the input according to its
internal similarity metric, and this process is repeated for every item in the
dataset. The recommendation profile of the model is the number of times each
item showed up in the top-$n$ most similar items of the whole dataset.

For example, a recommendation algorithm trained on the present version of the
MovieLens dataset would generate a list of $n$ movies for each input, creating a
list of $30,689 \times n$ movies. After this computationally intensive
calculation, the occurrence of each ID would be counted and ranked accordingly
to facilitate interpretation of results, that is, the movie ranked number 1
would be the movie featured the most times in the set of all recommendations,
and so forth for every other rank. From now on, $n$ will be fixed to 10.

\begin{figure}
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{1a_random}
    \caption{Trivial recommendations.\label{fig:fig1a}}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{1b_random_log}
    \caption{Log-log plot.\label{fig:fig1b}}
  \end{subfigure}
  \caption{Recommendation profile for the trivial model (a) and log-log plot
    (b).\label{fig:fig1}}
\end{figure}

The baseline against which all models will be compared can be seen in
Figure~\ref{fig:fig1}, the so called ``trivial model''. This model is a simple
sampler that returns $n$ movies at random when asked for a recommendation and,
thus, its recommendation profile averaged, evidently, $n$, with an appearance
very similar to that of the CDF of the normal distribution. The ``most
recommended'' movie appeared 25 times in the final list, while the ``least
recommended'' movie did not appear at all. The log-log plot
(Figure~\ref{fig:fig1b}), despite seeming out of place, will make sense when the
second model is presented.

Besides the trivial model, the simplest model that excludes user information is
the content-based recommender. In the real world this is an algorithm that is
able to identify similar items based on their metadata (description, tags, etc.)
and suggest the closest items to the one being purchased or viewed. A
straightforward way of building such an algorithm is creating a vector
representation of each item and then using a similarity metric to recommend the
items most similar to the one in question. The chosen similarity metric was
cosine similarity because of its simplicity, robustness, and ubiquity
\citep{sarwar_item-based_2001}.

The main non-trivial model used in this study was the one that simply generated
vector representations for the full MovieLens dataset, without any modifications
(which is why it will henceforth be referred to as the ``vanilla'' model). The
metadata for each movie was made up of its keywords, main cast, director, and
genres. The vector transformation was very simple, with each position
representing one of the words of the corpus, and each element indicating how
many times that word appeared in the metadata for that movie. When the
recommendation for a movie was requested, the algorithm measured the cosine
similarity between it and every other movie, returning the IDs belonging to
the top $n = 10$ most similar vectors.

\begin{figure}
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{2a_vanilla}
    \caption{Rec. based on movie metadata.\label{fig:fig2a}}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{2b_vanilla_log}
    \caption{Log-log plot.\label{fig:fig2b}}
  \end{subfigure}
  \caption{Recommendation profile for the vanilla model (a) and log-log plot
    (b).\label{fig:fig2}}
\end{figure}

The recommendation profile for the vanilla model can be seen if
Figure~\ref{fig:fig2a}. Compared to Figure~\ref{fig:fig1a}, the same
visualization for the trivial model, the distribution of the vanilla model
differs immensely: the movie ranked number 1 appeared more than 2000 times in
the full list of recommendations, with an almost exponential decrease in the
number of appearances from then on. The log-log plots of both models
(Figure~\ref{fig:fig1b} and Figure~\ref{fig:fig2b}, respectively), makes it
clear that the vanilla model is close to exponential, while its trivial
counterpart is (evidently) normal.

\begin{figure}
  \centering
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{3a_cutoff_low}
    \caption{Cutoff $k = 2$.\label{fig:fig3a}}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{3b_cutoff_med}
    \caption{Cutoff $k = 5$.\label{fig:fig3b}}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{3c_cutoff_high}
    \caption{Cutoff $k = 8$.\label{fig:fig3c}}
  \end{subfigure}
  \caption{Recommendation profile for cutoff $k = 2$ (a), $5$ (b), and $8$
    (c).\label{fig:fig3}}
\end{figure}

% Precisa, sem falta, colocar uma distribuição das notas dos filmes para mostrar
% que, apesar de existirem filmes mais populares, eles não são exponencialmente
% mais populares. Isso justifica procurar outra explicação para a diferença
% entre o vanila e o trivial que não seja somente o rating dos filmes.

A potential explanation for the difference between trivial and vanilla could
reside in the least used terms in the metadata: movies whose metadata share rare
words might have been recommended less frequently than movies whose metadata is
not so unique. To test this hypothesis, a cutoff point was created for words to
be included in the vector representation of the movies. Three cutoff points were
tested where only words with an absolute frequency larger than or equal to $k$,
$k = {2, 5, 8}$, could be included in the vector representations. The results
can be seen in Figure~\ref{fig:fig3} and, aside from variations in the
$y$-intercept, all plots are qualitatively very similar to
Figure~\ref{fig:fig2a}, indicating that rare words probably are not to blame for
the exponential-like decay.

\begin{figure}
  \centering
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{4a_cosine}
    \caption{Cosine distance.\label{fig:fig4a}}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{4b_euclidean}
    \caption{Euclidean distance.\label{fig:fig4b}}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{4c_manhattan}
    \caption{Manhattan distance.\label{fig:fig4c}}
  \end{subfigure}
  \caption{Recommendation profile for cosine (a), euclidean (b), and manhattan
    (c) distances.\label{fig:fig4}}
\end{figure}

The second validation experiment involved attempting to use other distance
metrics instead of cosine similarity \citep{ricci_introduction_2011}, since that
could also be a source of the strange behavior of the recommendation profile.
The goal here was verifying whether other metrics could do a better job at not
creating a subset of movies that ended up exponentially more recommended than
the rest. As attested by Figure~\ref{fig:fig4}, this was not the case.

At this point it is safe to say that the type of decay seen in recommendation
frequencies up until now is not spurious and must have a clear cause. A
hypothesis that is later mostly confirmed involves the average number of
non-zero elements in the vector representation of the movies: the sparser the
vectors, the higher the odds of the recommendation curve displaying a steep
left-hand side. This would be almost equivalent to creating an ``inverse cutoff
point'', removing common words from the vector representations.

\begin{figure}
  \centering
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{5a_p}
    \caption{$P(w_{i}) = 1 \times \overline{P(w)}$.\label{fig:fig5a}}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{5b_p10}
    \caption{$P(w_{i}) = 10 \times \overline{P(w)}$.\label{fig:fig5b}}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{5c_p100}
    \caption{$P(w_{i}) = 100 \times \overline{P(w)}$.\label{fig:fig5c}}
  \end{subfigure}
  \caption{Recommendation profile of samples with
    $P(w_{i}) = C \times \overline{P(w)}$, $C = 1$ (a), $10$ (b), and $100$
    (c).\label{fig:fig5}}
\end{figure}

To test whether this hypothesis held water, ``random'' vector representations
were created. These representations were based on fictional metadata that were
comprised of words sampled at random from the full corpus of movies; the
probability that word $w_{i}$ occurred in the metadata of a fictional movie was
equal to the average probability that any word would occur in some arbitrary
metadata ($\overline{P(w)}$) times a costant $C$, $C = {1, 10, 100}$. More
simply, the probability of an element being non-zero in a random vector
representation was the average probability that an arbitrary element of the
vanilla representations was non-zero times $C$. The constant was added as a way
to create less sparse vectors and allow for comparisons between different
inverse cutoff points.

Figure~\ref{fig:fig5} displays the recommendation profiles for each different
$C$. Concretely, the figures are equivalent to creating random metadata for the
movies where the probability of a word occurring was approximately
$1.54 \times 10^{-4}$, $1.54 \times 10^{-3}$, and $1.54 \times 10^{-2}$
respectively and then training the recommendation models. The results do support
the aforementioned hypothesis since less sparse vectors indeed generated less
exponential decays.

\begin{figure}
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{6a_artificial_movie}
    \caption{Vanilla model with artificial movie in red.\label{fig:fig6a}}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{6b_long}
    \caption{Model with short vector representations.\label{fig:fig6b}}
  \end{subfigure}
  \caption{Recommendation profile for artificial movie (a) and short vector
    representations (b).\label{fig:fig6}}
\end{figure}

After the previous experiments, sanity checks were needed in order to guarantee
that the previous hypothesis was able generalize. The first check should verify
whether an artificial movie created as a combination of the metadata from other
movies favored by the recommendation algorithm would also be favored, while the
second should check whether shorter vectors would change the decay already
observed despite being as sparse as their longer counterparts.

Figure~\ref{fig:fig6} showcases the two sanity checks. Figure~\ref{fig:fig6a}
was a model trained with the vanilla dataset with the addition of the movie
highlighted in red. As expected, this movie also showed up in the
top-recommended subset. Figure~\ref{fig:fig6b} comes from a model trained on
randomly generated vector representations in a similar fashion to the ones in
Figure~\ref{fig:fig5}, except each vector could only have 15,000 elements
instead of 55,681 (as with the vanilla model). The patter observed before
persisted, meaning that the hypothesis still stood.

\begin{figure}
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{7a_books}
    \caption{Recommendation profile for book dataset.\label{fig:fig7a}}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{7b_mimic}
    \caption{Random simularion of vanilla.\label{fig:fig7b}}
  \end{subfigure}
  \caption{Recommendation profile for book dataset (a) and random simulation of
    vanilla (b).\label{fig:fig7}}
\end{figure}

The last two models were considered the confirmations of the hypothesis that (at
least for this kind of recommendation systems) a subset of items was always much
more recommended than the rest as long as the data was sparse.
Figure~\ref{fig:fig7a} represents the same recommendation algorithm applied to
another dataset, the Book-Crossing dataset. Figure~\ref{fig:fig7b} contains the
results of the model applied to another set of random vector representations,
this time with the probability of each element being non-zero respecting the
marginal distributions of the vanilla dataset. Again, the exponential decay
pattern persisted, only slightly less pronounced in the Book-Crossing case.

The analysis up until now has been static, that is, the recommendation model is
trained and applyied to every movie in the dataset. There is no interaction with
users and no opportunity to evolve over time. The next chapter adresses this
point by using Google's newly released TensorFlow Recommenders library
\citep{noauthor_tensorflow_nodate} to gather data about what happens to a
system's recommendations as users follow its suggestions. Employing a deep
learning model that is able to improve over time is a significant departure from
the content-based models presented here and, if a similar recommendation profile
can also be detected for multi-criteria recommender systems on dynamic
scenarios, then the hypothesis ventilated in the section above would become even
more plausible.
